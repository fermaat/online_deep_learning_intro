{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generating text with Recurrent Neural Networks\n",
    "\n",
    "In this notebook we will make use of the Recurrent Neural Networks to make sequence predictions. We will use the book \"The Three Musketeers\" by Alexandre Dumas as our dataset and we will predict characters in order to generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reading\n",
    "Load the file from the data folder and inspect it. Standardize to lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"../data/musquetairesShort\"\n",
    "raw_text = open(filename).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = raw_text.lower()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Text preparation\n",
    "We create a set with the different characters and two dictionaries from indices to chars\n",
    "<font color=red><b>Generate dictionaries for the char to indices and indices to chars.\n",
    "<br>_Hint: use the enumerate function on the chars set_</b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "## Add code here\n",
    "# char_indices = \n",
    "# indices_char = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we generate the input and output arrays:\n",
    "\n",
    "The input will consist on sentences of a fixed (_maxlen_) lenght, while the outputs will be the next characters in the text.\n",
    "\n",
    "So, if the text is \"Welcome to Big Data Spain\" with _maxlen_ = 5, we will have:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Input = [\n",
    "     w, e, l, c, o,\n",
    "     e, l, c, o, m,\n",
    "     l, c, o, m, e,\n",
    "     ...\n",
    "    ]\n",
    "\n",
    "Output = [\n",
    "    m,\n",
    "    e, \n",
    "     ,\n",
    "     ...\n",
    "     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid overfitting (and improve performances) we can add a _step_ to the structure so that with step = 3, for example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input = [\n",
    "     w, e, l, c, o,\n",
    "     c, o, m, e,  , \n",
    "     m, e,  , t, o, \n",
    "     ...\n",
    "    ]\n",
    "\n",
    "Output = [\n",
    "    m,\n",
    "    t, \n",
    "     ,\n",
    "     ...\n",
    "     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>Fill the sentences and next_char lists with the input and output data</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    # Add code here\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next_chars[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dataset generation\n",
    "We turn the text into one-hot-like vectors. Initialize the Input and output arrays to zero as boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "Y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    Y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"timesteps = \", len (X[0]), \", numchars = \", len (X[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generation\n",
    "Build the LSTM model to be trained train on the data, on this config:\n",
    "- LSTM layer, with 256 units\n",
    "- LSTM layer, with 256 units\n",
    "- Dense layer, with 64 units\n",
    "- Dense softmax layer\n",
    "- On compilation, use adam as the optimizer and categorical_crossentropy as the loss function.\n",
    "- Print the summary\n",
    "\n",
    "\n",
    "<font color=red><b>Remember to initialize it propperly and to include input_shape on the first layer. <br> Hints: input_shape= (maxlen, len(chars))\n",
    "- Use the imported libraries</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.keras.backend.clear_session() \n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Train the model for two of epochs and see how it works. Use a batch_size of 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model Evaluation\n",
    "Let's test our model. In order to obtain a probabilistic answer we can sample from a probability array instead of just taking the max argument:\n",
    "\n",
    "<font color=red><b> Sometimes probabilities are rounded. Apply a normalization-like tratment to them in order to avoid this when sampling</b> </font>\n",
    "\n",
    "\n",
    "$$ p_i = \\frac{p_i}{\\sum_j p_j}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Build a function to get the next predicted index:\n",
    "def sample(preds, sample = True):\n",
    "    # take a sample from the probabilities\n",
    "    if sample:\n",
    "        # probs can be rounded and not sum up to one. Recalculate the probs in order to avoid this\n",
    "        # Add code here\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "    else:\n",
    "        probas = preds\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a seed in order to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "generated = ''\n",
    "sentence = text[start_index: start_index + maxlen]\n",
    "generated += sentence\n",
    "print (generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions\n",
    "This will be the secuence for which we are going to predict the next character:\n",
    "\n",
    "<font color=red> <b> Predict the next character given the input x_pred. <br>Hint: remember to take the first item in list</b>  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Predict next character given a model and the sequence to predict \n",
    "def get_next_char (model, x_pred, indices_char, Sample = True):\n",
    "    preds = ## Add code here\n",
    "    next_index = sample(preds, 1.0)\n",
    "    return indices_char[next_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "for t, char in enumerate(sentence):\n",
    "    x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "Sample = True\n",
    "# this gets the next character     \n",
    "next_char = get_next_char (model, x_pred, indices_char, Sample)  \n",
    "\n",
    "print (next_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict some more characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "sentence = text[start_index: start_index + maxlen]\n",
    "print('Seed: ' + sentence + '\"')\n",
    "print('---------------------- Generated Text -----------------------')\n",
    "chars_to_predict = 400\n",
    "for i in range(chars_to_predict):\n",
    "    x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    next_char = get_next_char (model, x_pred, indices_char, Sample)\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a trained model\n",
    "Training Deep Learning Models is time consuming. So, some pretrained models are available to be loaded and take a look at better predictions. We will load a model for each 5 epochs in order to see the evolution. \n",
    "\n",
    "<font color=red> <b> Load a model for each time and predict the text <br> Hint: You can load the whole model or just the weights as the configuration is the same</b>  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "partial_n_epoch = 5\n",
    "times = 12\n",
    "\n",
    "np.random.seed (1)\n",
    "for j in range (times):\n",
    "    \n",
    "    count += partial_n_epoch\n",
    "    print (\"\")\n",
    "    print (\"-------------- Next Model --------------\")\n",
    "    print (\"Trained on \", count, \" epochs\")\n",
    "    modelName = '../models/MusquetairesModelOptimizedMode_' + str (count) + '.h5' \n",
    "    ## Add code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
